{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import all required library\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "import datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 19\n",
    "#reproducibility\n",
    "def _seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    set_seed(seed)\n",
    "\n",
    "_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that can print the trainable parameters \n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params} \\nall model parameters: {all_model_params}{\"\\n\"}percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 17251 samples\n",
      "                                                    text  label\n",
      "8511   The value of using this technology to read stu...      0\n",
      "5837   the author supports his ideas very well becaus...      0\n",
      "17230  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
      "12936  Dear Mr./Mrs. Senator,\\n\\nThe Electoral Colleg...      0\n",
      "15253  \"The Challenge of Exploring Venus\" by James Tr...      1\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.read_csv(\"7-prompts.csv\")\n",
    "print(f\"We have {len(full_data)} samples\") # Number of data we have\n",
    "\n",
    "print(full_data.sample(5,random_state=19))\n",
    "\n",
    "# full_data = full_data.head(1000) # Subset the data for testing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 12075 training samples\n",
      "We have 5176 validation samples\n",
      "----------------------------\n",
      "Number of Essays written by Human: 14247\n",
      "Number of Essays generated by LLM: 3004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split it when augmented data is ready\n",
    "X_train, X_val, y_train, y_val = train_test_split(full_data[\"text\"],\n",
    "                                                  full_data[\"label\"],\n",
    "                                                  test_size=0.3,\n",
    "                                                  stratify=full_data[\"label\"],\n",
    "                                                  random_state=42)\n",
    "print(f\"We have {len(X_train)} training samples\")\n",
    "print(f\"We have {len(X_val)} validation samples\")\n",
    "print(\"----------------------------\")\n",
    "count = full_data[\"label\"].value_counts()\n",
    "print(f\"Number of Essays written by Human: {count[0]}\")\n",
    "print(f\"Number of Essays generated by LLM: {count[1]}\")\n",
    "\n",
    "X_train.reset_index(drop = True, inplace = True)\n",
    "y_train.reset_index(drop = True, inplace = True)\n",
    "X_val.reset_index(drop = True, inplace = True)\n",
    "y_val.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 66955010 \n",
      "all model parameters: 66955010\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load DistilBERT model and tokenizer (uncased)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", return_dict=True, num_labels=2\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "print(print_number_of_trainable_model_parameters(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank Number\n",
    "    lora_alpha=32, # Alpha (Scaling Factor)\n",
    "    lora_dropout=0.05, # Dropout Prob for Lora\n",
    "    target_modules=[\"q_lin\", \"k_lin\",\"v_lin\"], # Which layer to apply LoRA, usually only apply on MultiHead Attention Layer\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_CLS, # Seqence to Classification Task\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 923906 \n",
      "all model parameters: 67878916\n",
      "percentage of trainable model parameters: 1.36%\n"
     ]
    }
   ],
   "source": [
    "# Get our LoRA-enabled model\n",
    "peft_model = get_peft_model(model, \n",
    "                            lora_config,)\n",
    "\n",
    "# Reduced trainble parameters\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): MultiHeadSelfAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_V): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_embedding_V): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_V): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_embedding_V): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_V): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_embedding_V): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize_func(data):\n",
    "    return tokenizer(\n",
    "            data['texts'],\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12075/12075 [00:02<00:00, 4293.53 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 12075\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the Training Data\n",
    "train_dataset = datasets.Dataset.from_pandas(pd.DataFrame({\"texts\":X_train,\"labels\":y_train}))\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_func,\n",
    "    batched=True,\n",
    "    remove_columns=[\"texts\"]\n",
    ")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5176/5176 [00:01<00:00, 4336.27 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5176\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the Validation Data\n",
    "val_dataset = datasets.Dataset.from_pandas(pd.DataFrame({\"texts\":X_val,\"labels\":y_val}))\n",
    "val_dataset = val_dataset.map(\n",
    "    tokenize_func,\n",
    "    batched=True,\n",
    "    remove_columns=[\"texts\"]\n",
    ")\n",
    "\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps: 378\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/756 00:00 < 01:47, 6.99 it/s, Epoch 0.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n",
      "not using dora\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m peft_model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/peft-distilbert-lora-local\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mpeft_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# peft_trainer.model.save_pretrained(peft_model_path) # Save the fine-tuned model\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# tokenizer.save_pretrained(peft_model_path) # Save the tokenizer\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fl-lora/lib/python3.12/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fl-lora/lib/python3.12/site-packages/transformers/trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define Eval Metric\n",
    "def metrics(eval_prediction):\n",
    "    logits, labels = eval_prediction\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "    auc_score = roc_auc_score(labels, pred)\n",
    "    return {\"Val-AUC\": auc_score}\n",
    "\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "# Define training Args\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir='./result-distilbert-lora',\n",
    "    logging_dir='./logs-distilbert-lora',\n",
    "#     auto_find_batch_size=True,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=train_batch_size, # You can adjust this value base on your available GPU, You may encounter \"out of memory\" error if this value is too lartge\n",
    "    per_device_eval_batch_size=eval_batch_size, # You can adjust this value base on your available GPU, You may encounter \"out of memory\" error if this value is too lartge\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    seed=42,\n",
    "    fp16=True, # Only use with GPU\n",
    "    report_to='none'\n",
    ")   \n",
    "\n",
    "# Define Optimzer\n",
    "optimizer = AdamW(peft_model.parameters(), \n",
    "                  lr=1e-4,\n",
    "                  no_deprecation_warning=True)\n",
    "\n",
    "# Define Scheduler\n",
    "n_epochs = peft_training_args.num_train_epochs\n",
    "total_steps = n_epochs * math.ceil(len(train_dataset) / train_batch_size / 2)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps)\n",
    "\n",
    "# Data Collator\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, \n",
    "    padding=\"longest\"\n",
    ")\n",
    "\n",
    "\n",
    "# Define Trainer\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=train_dataset, # Training Data\n",
    "    eval_dataset=val_dataset, # Evaluation Data\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=metrics,\n",
    "    optimizers=(optimizer,lr_scheduler),\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "print(f\"Total Steps: {total_steps}\")\n",
    "\n",
    "# Path to save the fine-tuned model\n",
    "peft_model_path=\"/kaggle/working/peft-distilbert-lora-local\"\n",
    "\n",
    "# Train the model\n",
    "peft_trainer.train()\n",
    "\n",
    "# peft_trainer.model.save_pretrained(peft_model_path) # Save the fine-tuned model\n",
    "# tokenizer.save_pretrained(peft_model_path) # Save the tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
